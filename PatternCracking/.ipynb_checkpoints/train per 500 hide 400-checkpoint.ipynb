{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"C:\\\\Users\\\\nilu1\\\\Desktop\\\\Code\\\\AndroidPattern\\\\Data\\\\Spike\\\\Using\\\\\"\n",
    "path=\"C:\\\\Users\\\\nilu1\\\\Desktop\\\\Code\\\\AndroidPattern\\\\Data\\\\Using\\\\\"\n",
    "pathex = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "score=[]\n",
    "score12=[]\n",
    "# modelpath=\"C:\\\\Users\\\\nilu1\\\\Desktop\\\\Code\\\\AndroidPattern\\\\Data\\\\Spike\\\\Models\\\\\"\n",
    "# modelpath=\"C:\\\\Users\\\\nilu1\\\\Desktop\\\\Code\\\\AndroidPattern\\\\Data\\\\Models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2724\n",
      "6\n",
      "2451600 600000\n",
      "Iteration 1, loss = 2.34368239\n",
      "Iteration 2, loss = 2.30657928\n",
      "Iteration 3, loss = 2.30365459\n",
      "Iteration 4, loss = 2.30288183\n",
      "Iteration 5, loss = 2.30078469\n",
      "Iteration 6, loss = 2.29894690\n",
      "Iteration 7, loss = 2.29764996\n",
      "Iteration 8, loss = 2.29585606\n",
      "Iteration 9, loss = 2.29432338\n",
      "Iteration 10, loss = 2.29339826\n",
      "Iteration 11, loss = 2.29244713\n",
      "Iteration 12, loss = 2.29083862\n",
      "Iteration 13, loss = 2.28901899\n",
      "Iteration 14, loss = 2.28737757\n",
      "Iteration 15, loss = 2.28558738\n",
      "Iteration 16, loss = 2.28356188\n",
      "Iteration 17, loss = 2.28150706\n",
      "Iteration 18, loss = 2.27933835\n",
      "Iteration 19, loss = 2.27612223\n",
      "Iteration 20, loss = 2.27312145\n",
      "Iteration 21, loss = 2.27046698\n",
      "Iteration 22, loss = 2.26749389\n",
      "Iteration 23, loss = 2.26618550\n",
      "Iteration 24, loss = 2.26703302\n",
      "Iteration 25, loss = 2.25887571\n",
      "Iteration 26, loss = 2.26016435\n",
      "Iteration 27, loss = 2.25306257\n",
      "Iteration 28, loss = 2.25119895\n",
      "Iteration 29, loss = 2.24854661\n",
      "Iteration 30, loss = 2.24445365\n",
      "Iteration 31, loss = 2.23897390\n",
      "Iteration 32, loss = 2.23701177\n",
      "Iteration 33, loss = 2.23301245\n",
      "Iteration 34, loss = 2.22881427\n",
      "Iteration 35, loss = 2.22427486\n",
      "Iteration 36, loss = 2.22078726\n",
      "Iteration 37, loss = 2.21715881\n",
      "Iteration 38, loss = 2.21249901\n",
      "Iteration 39, loss = 2.20583000\n",
      "Iteration 40, loss = 2.20287215\n",
      "Iteration 41, loss = 2.19530299\n",
      "Iteration 42, loss = 2.18797672\n",
      "Iteration 43, loss = 2.18006625\n",
      "Iteration 44, loss = 2.17168625\n",
      "Iteration 45, loss = 2.16389266\n",
      "Iteration 46, loss = 2.15960940\n",
      "Iteration 47, loss = 2.15817122\n",
      "Iteration 48, loss = 2.14858574\n",
      "Iteration 49, loss = 2.13527585\n",
      "Iteration 50, loss = 2.12696107\n",
      "Iteration 51, loss = 2.11085555\n",
      "Iteration 52, loss = 2.10451940\n",
      "Iteration 53, loss = 2.09485452\n",
      "Iteration 54, loss = 2.08367641\n",
      "Iteration 55, loss = 2.07396261\n",
      "Iteration 56, loss = 2.06323120\n",
      "Iteration 57, loss = 2.05439903\n",
      "Iteration 58, loss = 2.04555333\n",
      "Iteration 59, loss = 2.03681006\n",
      "Iteration 60, loss = 2.02739363\n",
      "Iteration 61, loss = 2.02445006\n",
      "Iteration 62, loss = 2.01454341\n",
      "Iteration 63, loss = 2.00386307\n",
      "Iteration 64, loss = 1.99741207\n",
      "Iteration 65, loss = 1.99077191\n",
      "Iteration 66, loss = 1.98062048\n",
      "Iteration 67, loss = 1.97065044\n",
      "Iteration 68, loss = 1.96495163\n",
      "Iteration 69, loss = 1.95612598\n",
      "Iteration 70, loss = 1.94784263\n",
      "Iteration 71, loss = 1.94032776\n",
      "Iteration 72, loss = 1.93283593\n",
      "Iteration 73, loss = 1.92839051\n",
      "Iteration 74, loss = 1.92625992\n",
      "Iteration 75, loss = 1.91922674\n",
      "Iteration 76, loss = 1.90770202\n",
      "Iteration 77, loss = 1.89782252\n",
      "Iteration 78, loss = 1.89451968\n",
      "Iteration 79, loss = 1.88650897\n",
      "Iteration 80, loss = 1.88128479\n",
      "Iteration 81, loss = 1.87037816\n",
      "Iteration 82, loss = 1.86594738\n",
      "Iteration 83, loss = 1.86954106\n",
      "Iteration 84, loss = 1.87973879\n",
      "Iteration 85, loss = 1.86910492\n",
      "Iteration 86, loss = 1.86157530\n",
      "Iteration 87, loss = 1.85163333\n",
      "Iteration 88, loss = 1.83871006\n",
      "Iteration 89, loss = 1.82795619\n",
      "Iteration 90, loss = 1.82020747\n",
      "Iteration 91, loss = 1.82049175\n",
      "Iteration 92, loss = 1.81288917\n",
      "Iteration 93, loss = 1.80408429\n",
      "Iteration 94, loss = 1.80731378\n",
      "Iteration 95, loss = 1.79743830\n",
      "Iteration 96, loss = 1.78863512\n",
      "Iteration 97, loss = 1.78662713\n",
      "Iteration 98, loss = 1.78328149\n",
      "Iteration 99, loss = 1.77591691\n",
      "Iteration 100, loss = 1.77100561\n",
      "Iteration 101, loss = 1.76697407\n",
      "Iteration 102, loss = 1.76130015\n",
      "Iteration 103, loss = 1.75796731\n",
      "Iteration 104, loss = 1.75660203\n",
      "Iteration 105, loss = 1.75186514\n",
      "Iteration 106, loss = 1.74466935\n",
      "Iteration 107, loss = 1.74606692\n",
      "Iteration 108, loss = 1.74192133\n",
      "Iteration 109, loss = 1.73559202\n",
      "Iteration 110, loss = 1.73357460\n",
      "Iteration 111, loss = 1.72345461\n",
      "Iteration 112, loss = 1.71676715\n",
      "Iteration 113, loss = 1.71374837\n",
      "Iteration 114, loss = 1.71037219\n",
      "Iteration 115, loss = 1.70409751\n",
      "Iteration 116, loss = 1.70453786\n",
      "Iteration 117, loss = 1.70366623\n",
      "Iteration 118, loss = 1.70585100\n",
      "Iteration 119, loss = 1.69846358\n",
      "Iteration 120, loss = 1.68786485\n",
      "Iteration 121, loss = 1.68684154\n",
      "Iteration 122, loss = 1.68670127\n",
      "Iteration 123, loss = 1.67587517\n",
      "Iteration 124, loss = 1.67423294\n",
      "Iteration 125, loss = 1.67347303\n",
      "Iteration 126, loss = 1.66344020\n",
      "Iteration 127, loss = 1.66163908\n",
      "Iteration 128, loss = 1.65792849\n",
      "Iteration 129, loss = 1.65280814\n",
      "Iteration 130, loss = 1.65337099\n",
      "Iteration 131, loss = 1.65083436\n",
      "Iteration 132, loss = 1.66040067\n",
      "Iteration 133, loss = 1.64546966\n",
      "Iteration 134, loss = 1.63697222\n",
      "Iteration 135, loss = 1.63592622\n",
      "Iteration 136, loss = 1.63345299\n",
      "Iteration 137, loss = 1.63690862\n",
      "Iteration 138, loss = 1.63247320\n",
      "Iteration 139, loss = 1.64630345\n",
      "Iteration 140, loss = 1.64106244\n",
      "Iteration 141, loss = 1.62649644\n",
      "Iteration 142, loss = 1.63279671\n",
      "Iteration 143, loss = 1.63059173\n",
      "Iteration 144, loss = 1.61886476\n",
      "Iteration 145, loss = 1.62625276\n",
      "Iteration 146, loss = 1.61139335\n",
      "Iteration 147, loss = 1.60402353\n",
      "Iteration 148, loss = 1.59191710\n",
      "Iteration 149, loss = 1.59914838\n",
      "Iteration 150, loss = 1.58494119\n",
      "Iteration 151, loss = 1.58665989\n",
      "Iteration 152, loss = 1.57824406\n",
      "Iteration 153, loss = 1.57115627\n",
      "Iteration 154, loss = 1.57096976\n",
      "Iteration 155, loss = 1.56208923\n",
      "Iteration 156, loss = 1.57487385\n",
      "Iteration 157, loss = 1.58672732\n",
      "Iteration 158, loss = 1.56301272\n",
      "Iteration 159, loss = 1.57176796\n",
      "Iteration 160, loss = 1.54942628\n",
      "Iteration 161, loss = 1.54291104\n",
      "Iteration 162, loss = 1.53856461\n",
      "Iteration 163, loss = 1.53341258\n",
      "Iteration 164, loss = 1.54276101\n",
      "Iteration 165, loss = 1.53103199\n",
      "Iteration 166, loss = 1.52402604\n",
      "Iteration 167, loss = 1.52444162\n",
      "Iteration 168, loss = 1.53552164\n",
      "Iteration 169, loss = 1.52057468\n",
      "Iteration 170, loss = 1.51374777\n",
      "Iteration 171, loss = 1.51553739\n",
      "Iteration 172, loss = 1.51127206\n",
      "Iteration 173, loss = 1.50449978\n",
      "Iteration 174, loss = 1.49937821\n",
      "Iteration 175, loss = 1.49202707\n",
      "Iteration 176, loss = 1.49794214\n",
      "Iteration 177, loss = 1.49151740\n",
      "Iteration 178, loss = 1.48933589\n",
      "Iteration 179, loss = 1.48594402\n",
      "Iteration 180, loss = 1.47808747\n",
      "Iteration 181, loss = 1.47959085\n",
      "Iteration 182, loss = 1.47397780\n",
      "Iteration 183, loss = 1.47514989\n",
      "Iteration 184, loss = 1.47288390\n",
      "Iteration 185, loss = 1.46169650\n",
      "Iteration 186, loss = 1.46654632\n",
      "Iteration 187, loss = 1.46396084\n",
      "Iteration 188, loss = 1.45605239\n",
      "Iteration 189, loss = 1.45433573\n",
      "Iteration 190, loss = 1.45104702\n",
      "Iteration 191, loss = 1.44521648\n",
      "Iteration 192, loss = 1.44315785\n",
      "Iteration 193, loss = 1.43990755\n",
      "Iteration 194, loss = 1.44361268\n",
      "Iteration 195, loss = 1.43292912\n",
      "Iteration 196, loss = 1.42884180\n",
      "Iteration 197, loss = 1.42583261\n",
      "Iteration 198, loss = 1.42173490\n",
      "Iteration 199, loss = 1.42501759\n",
      "Iteration 200, loss = 1.42078153\n",
      "Iteration 201, loss = 1.42369827\n",
      "Iteration 202, loss = 1.41085811\n",
      "Iteration 203, loss = 1.41373955\n",
      "Iteration 204, loss = 1.41036016\n",
      "Iteration 205, loss = 1.40803943\n",
      "Iteration 206, loss = 1.41856570\n",
      "Iteration 207, loss = 1.40866501\n",
      "Iteration 208, loss = 1.39916034\n",
      "Iteration 209, loss = 1.39550929\n",
      "Iteration 210, loss = 1.39237113\n",
      "Iteration 211, loss = 1.39143163\n",
      "Iteration 212, loss = 1.39062223\n",
      "Iteration 213, loss = 1.40039884\n",
      "Iteration 214, loss = 1.39587553\n",
      "Iteration 215, loss = 1.39496212\n",
      "Iteration 216, loss = 1.40517726\n",
      "Iteration 217, loss = 1.37662345\n",
      "Iteration 218, loss = 1.38565637\n",
      "Iteration 219, loss = 1.39844622\n",
      "Iteration 220, loss = 1.37976645\n",
      "Iteration 221, loss = 1.38596576\n",
      "Iteration 222, loss = 1.38046787\n",
      "Iteration 223, loss = 1.38665020\n",
      "Iteration 224, loss = 1.37024974\n",
      "Iteration 225, loss = 1.35623527\n",
      "Iteration 226, loss = 1.35226790\n",
      "Iteration 227, loss = 1.35452942\n",
      "Iteration 228, loss = 1.35409892\n",
      "Iteration 229, loss = 1.34791348\n",
      "Iteration 230, loss = 1.34883434\n",
      "Iteration 231, loss = 1.35700696\n",
      "Iteration 232, loss = 1.34712158\n",
      "Iteration 233, loss = 1.33385918\n",
      "Iteration 234, loss = 1.33142168\n",
      "Iteration 235, loss = 1.33124762\n",
      "Iteration 236, loss = 1.32735843\n",
      "Iteration 237, loss = 1.33156591\n",
      "Iteration 238, loss = 1.32269576\n",
      "Iteration 239, loss = 1.32794104\n",
      "Iteration 240, loss = 1.33417264\n",
      "Iteration 241, loss = 1.32004393\n",
      "Iteration 242, loss = 1.31582231\n",
      "Iteration 243, loss = 1.31595286\n",
      "Iteration 244, loss = 1.31377491\n",
      "Iteration 245, loss = 1.31751109\n",
      "Iteration 246, loss = 1.31301604\n",
      "Iteration 247, loss = 1.29851784\n",
      "Iteration 248, loss = 1.29438552\n",
      "Iteration 249, loss = 1.29760465\n",
      "Iteration 250, loss = 1.29391019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251, loss = 1.29914305\n",
      "Iteration 252, loss = 1.29422294\n",
      "Iteration 253, loss = 1.29039917\n",
      "Iteration 254, loss = 1.28767291\n",
      "Iteration 255, loss = 1.28132999\n",
      "Iteration 256, loss = 1.27626964\n",
      "Iteration 257, loss = 1.27246048\n",
      "Iteration 258, loss = 1.27454489\n",
      "Iteration 259, loss = 1.26642366\n",
      "Iteration 260, loss = 1.26041379\n",
      "Iteration 261, loss = 1.26094725\n",
      "Iteration 262, loss = 1.26690815\n",
      "Iteration 263, loss = 1.26648236\n",
      "Iteration 264, loss = 1.26578372\n",
      "Iteration 265, loss = 1.25802036\n",
      "Iteration 266, loss = 1.25109665\n",
      "Iteration 267, loss = 1.25469648\n",
      "Iteration 268, loss = 1.25158301\n",
      "Iteration 269, loss = 1.24811286\n",
      "Iteration 270, loss = 1.24767918\n",
      "Iteration 271, loss = 1.24108361\n",
      "Iteration 272, loss = 1.24063919\n",
      "Iteration 273, loss = 1.24060049\n",
      "Iteration 274, loss = 1.23471545\n",
      "Iteration 275, loss = 1.22529813\n",
      "Iteration 276, loss = 1.23631554\n",
      "Iteration 277, loss = 1.22243907\n",
      "Iteration 278, loss = 1.22121422\n",
      "Iteration 279, loss = 1.23054490\n",
      "Iteration 280, loss = 1.23415103\n",
      "Iteration 281, loss = 1.22769606\n",
      "Iteration 282, loss = 1.21901790\n",
      "Iteration 283, loss = 1.22293719\n",
      "Iteration 284, loss = 1.21821243\n",
      "Iteration 285, loss = 1.24072673\n",
      "Iteration 286, loss = 1.24409120\n",
      "Iteration 287, loss = 1.25049604\n",
      "Iteration 288, loss = 1.22417557\n",
      "Iteration 289, loss = 1.21962331\n",
      "Iteration 290, loss = 1.22583331\n",
      "Iteration 291, loss = 1.20667827\n",
      "Iteration 292, loss = 1.19559207\n",
      "Iteration 293, loss = 1.18348540\n",
      "Iteration 294, loss = 1.18152858\n",
      "Iteration 295, loss = 1.17908917\n",
      "Iteration 296, loss = 1.17628645\n",
      "Iteration 297, loss = 1.17484363\n",
      "Iteration 298, loss = 1.17350090\n",
      "Iteration 299, loss = 1.17138361\n",
      "Iteration 300, loss = 1.17879399\n",
      "Iteration 301, loss = 1.17700104\n",
      "Iteration 302, loss = 1.17950201\n",
      "Iteration 303, loss = 1.16941205\n",
      "Iteration 304, loss = 1.17450695\n",
      "Iteration 305, loss = 1.17779533\n",
      "Iteration 306, loss = 1.15721452\n",
      "Iteration 307, loss = 1.16540758\n",
      "Iteration 308, loss = 1.14959271\n",
      "Iteration 309, loss = 1.15706644\n",
      "Iteration 310, loss = 1.15616368\n",
      "Iteration 311, loss = 1.16283119\n",
      "Iteration 312, loss = 1.14378322\n",
      "Iteration 313, loss = 1.14317662\n",
      "Iteration 314, loss = 1.13844432\n",
      "Iteration 315, loss = 1.14206880\n",
      "Iteration 316, loss = 1.13784426\n",
      "Iteration 317, loss = 1.13668442\n",
      "Iteration 318, loss = 1.15004544\n",
      "Iteration 319, loss = 1.14580367\n",
      "Iteration 320, loss = 1.13347954\n",
      "Iteration 321, loss = 1.12608196\n",
      "Iteration 322, loss = 1.13411628\n",
      "Iteration 323, loss = 1.12465285\n",
      "Iteration 324, loss = 1.15310845\n",
      "Iteration 325, loss = 1.13197383\n",
      "Iteration 326, loss = 1.11763214\n",
      "Iteration 327, loss = 1.11204599\n",
      "Iteration 328, loss = 1.10559060\n",
      "Iteration 329, loss = 1.10094897\n",
      "Iteration 330, loss = 1.10213354\n",
      "Iteration 331, loss = 1.09918172\n",
      "Iteration 332, loss = 1.09939584\n",
      "Iteration 333, loss = 1.09711805\n",
      "Iteration 334, loss = 1.09113504\n",
      "Iteration 335, loss = 1.09284007\n",
      "Iteration 336, loss = 1.10752935\n",
      "Iteration 337, loss = 1.09267654\n",
      "Iteration 338, loss = 1.10269891\n",
      "Iteration 339, loss = 1.10810184\n",
      "Iteration 340, loss = 1.09286278\n",
      "Iteration 341, loss = 1.08551827\n",
      "Iteration 342, loss = 1.08278518\n",
      "Iteration 343, loss = 1.08555859\n",
      "Iteration 344, loss = 1.07181684\n",
      "Iteration 345, loss = 1.07674044\n",
      "Iteration 346, loss = 1.07877838\n",
      "Iteration 347, loss = 1.06766880\n",
      "Iteration 348, loss = 1.07101802\n",
      "Iteration 349, loss = 1.07647445\n",
      "Iteration 350, loss = 1.08958358\n",
      "Iteration 351, loss = 1.08086212\n",
      "Iteration 352, loss = 1.07475105\n",
      "Iteration 353, loss = 1.07298619\n",
      "Iteration 354, loss = 1.06133480\n",
      "Iteration 355, loss = 1.06178111\n",
      "Iteration 356, loss = 1.05776817\n",
      "Iteration 357, loss = 1.04892530\n",
      "Iteration 358, loss = 1.04463794\n",
      "Iteration 359, loss = 1.04415851\n",
      "Iteration 360, loss = 1.04620366\n",
      "Iteration 361, loss = 1.04145983\n",
      "Iteration 362, loss = 1.03629796\n",
      "Iteration 363, loss = 1.03474760\n",
      "Iteration 364, loss = 1.03460646\n",
      "Iteration 365, loss = 1.03617684\n",
      "Iteration 366, loss = 1.02800715\n",
      "Iteration 367, loss = 1.02355431\n",
      "Iteration 368, loss = 1.02323969\n",
      "Iteration 369, loss = 1.02343397\n",
      "Iteration 370, loss = 1.02436493\n",
      "Iteration 371, loss = 1.02468999\n",
      "Iteration 372, loss = 1.03151681\n",
      "Iteration 373, loss = 1.02354266\n",
      "Iteration 374, loss = 1.01906234\n",
      "Iteration 375, loss = 1.01570713\n",
      "Iteration 376, loss = 1.01193717\n",
      "Iteration 377, loss = 1.01230000\n",
      "Iteration 378, loss = 1.01226758\n",
      "Iteration 379, loss = 1.01779502\n",
      "Iteration 380, loss = 1.02041116\n",
      "Iteration 381, loss = 1.00782076\n",
      "Iteration 382, loss = 1.00020016\n",
      "Iteration 383, loss = 1.00408555\n",
      "Iteration 384, loss = 0.99580567\n",
      "Iteration 385, loss = 0.99929429\n",
      "Iteration 386, loss = 0.99324197\n",
      "Iteration 387, loss = 0.99256327\n",
      "Iteration 388, loss = 0.99137159\n",
      "Iteration 389, loss = 0.99178855\n",
      "Iteration 390, loss = 0.98503109\n",
      "Iteration 391, loss = 0.98047691\n",
      "Iteration 392, loss = 0.98510804\n",
      "Iteration 393, loss = 0.97903001\n",
      "Iteration 394, loss = 0.97449202\n",
      "Iteration 395, loss = 0.97444431\n",
      "Iteration 396, loss = 0.97956615\n",
      "Iteration 397, loss = 0.97711545\n",
      "Iteration 398, loss = 0.98645526\n",
      "Iteration 399, loss = 0.97207990\n",
      "Iteration 400, loss = 0.96583354\n",
      "Iteration 401, loss = 0.95965922\n",
      "Iteration 402, loss = 0.95909978\n",
      "Iteration 403, loss = 0.95803537\n",
      "Iteration 404, loss = 0.96094053\n",
      "Iteration 405, loss = 0.96121747\n",
      "Iteration 406, loss = 0.96034790\n",
      "Iteration 407, loss = 0.96283376\n",
      "Iteration 408, loss = 0.95481695\n",
      "Iteration 409, loss = 0.95522719\n",
      "Iteration 410, loss = 0.95580711\n",
      "Iteration 411, loss = 0.95316675\n",
      "Iteration 412, loss = 0.94412140\n",
      "Iteration 413, loss = 0.94400532\n",
      "Iteration 414, loss = 0.94457891\n",
      "Iteration 415, loss = 0.93762499\n",
      "Iteration 416, loss = 0.94311525\n",
      "Iteration 417, loss = 0.94348767\n",
      "Iteration 418, loss = 0.95335217\n",
      "Iteration 419, loss = 0.94406668\n",
      "Iteration 420, loss = 0.94612729\n",
      "Iteration 421, loss = 0.93314504\n",
      "Iteration 422, loss = 0.93154456\n",
      "Iteration 423, loss = 0.93571706\n",
      "Iteration 424, loss = 0.93114080\n",
      "Iteration 425, loss = 0.92429739\n",
      "Iteration 426, loss = 0.92872485\n",
      "Iteration 427, loss = 0.92359217\n",
      "Iteration 428, loss = 0.92455805\n",
      "Iteration 429, loss = 0.92562339\n",
      "Iteration 430, loss = 0.92608765\n",
      "Iteration 431, loss = 0.92145268\n",
      "Iteration 432, loss = 0.90976372\n",
      "Iteration 433, loss = 0.91972847\n",
      "Iteration 434, loss = 0.90263467\n",
      "Iteration 435, loss = 0.90372549\n",
      "Iteration 436, loss = 0.91358403\n",
      "Iteration 437, loss = 0.90637772\n",
      "Iteration 438, loss = 0.89987368\n",
      "Iteration 439, loss = 0.89809074\n",
      "Iteration 440, loss = 0.89366633\n",
      "Iteration 441, loss = 0.88873302\n",
      "Iteration 442, loss = 0.88662285\n",
      "Iteration 443, loss = 0.88747325\n",
      "Iteration 444, loss = 0.90263253\n",
      "Iteration 445, loss = 0.88296972\n",
      "Iteration 446, loss = 0.88401460\n",
      "Iteration 447, loss = 0.88631222\n",
      "Iteration 448, loss = 0.88552206\n",
      "Iteration 449, loss = 0.87937518\n",
      "Iteration 450, loss = 0.87930174\n",
      "Iteration 451, loss = 0.86999841\n",
      "Iteration 452, loss = 0.87059763\n",
      "Iteration 453, loss = 0.87317716\n",
      "Iteration 454, loss = 0.86844199\n",
      "Iteration 455, loss = 0.88092470\n",
      "Iteration 456, loss = 0.88741999\n",
      "Iteration 457, loss = 0.88212809\n",
      "Iteration 458, loss = 0.86604272\n",
      "Iteration 459, loss = 0.87050105\n",
      "Iteration 460, loss = 0.85763212\n",
      "Iteration 461, loss = 0.85578104\n",
      "Iteration 462, loss = 0.86189391\n",
      "Iteration 463, loss = 0.85751553\n",
      "Iteration 464, loss = 0.85166470\n",
      "Iteration 465, loss = 0.85294650\n",
      "Iteration 466, loss = 0.84929618\n",
      "Iteration 467, loss = 0.84319363\n",
      "Iteration 468, loss = 0.84388131\n",
      "Iteration 469, loss = 0.84310602\n",
      "Iteration 470, loss = 0.84120035\n",
      "Iteration 471, loss = 0.85033871\n",
      "Iteration 472, loss = 0.84839304\n",
      "Iteration 473, loss = 0.85790181\n",
      "Iteration 474, loss = 0.84380916\n",
      "Iteration 475, loss = 0.84465643\n",
      "Iteration 476, loss = 0.83780226\n",
      "Iteration 477, loss = 0.83568424\n",
      "Iteration 478, loss = 0.83626553\n",
      "Iteration 479, loss = 0.83411180\n",
      "Iteration 480, loss = 0.82997861\n",
      "Iteration 481, loss = 0.82939248\n",
      "Iteration 482, loss = 0.82410725\n",
      "Iteration 483, loss = 0.82862928\n",
      "Iteration 484, loss = 0.83492283\n",
      "Iteration 485, loss = 0.82189175\n",
      "Iteration 486, loss = 0.82851027\n",
      "Iteration 487, loss = 0.83342804\n",
      "Iteration 488, loss = 0.83685536\n",
      "Iteration 489, loss = 0.82406065\n",
      "Iteration 490, loss = 0.82987461\n",
      "Iteration 491, loss = 0.81248285\n",
      "Iteration 492, loss = 0.81182960\n",
      "Iteration 493, loss = 0.80708752\n",
      "Iteration 494, loss = 0.80661750\n",
      "Iteration 495, loss = 0.79938359\n",
      "Iteration 496, loss = 0.80056053\n",
      "Iteration 497, loss = 0.79547360\n",
      "Iteration 498, loss = 0.79520305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 499, loss = 0.79710492\n",
      "Iteration 500, loss = 0.79065596\n",
      "Iteration 501, loss = 0.78621699\n",
      "Iteration 502, loss = 0.78849186\n",
      "Iteration 503, loss = 0.78939834\n",
      "Iteration 504, loss = 0.78831965\n",
      "Iteration 505, loss = 0.78854427\n",
      "Iteration 506, loss = 0.78441236\n",
      "Iteration 507, loss = 0.78669750\n",
      "Iteration 508, loss = 0.78472377\n",
      "Iteration 509, loss = 0.78482516\n",
      "Iteration 510, loss = 0.78547229\n",
      "Iteration 511, loss = 0.78114478\n",
      "Iteration 512, loss = 0.77808443\n",
      "Iteration 513, loss = 0.78489096\n",
      "Iteration 514, loss = 0.77545117\n",
      "Iteration 515, loss = 0.76631776\n",
      "Iteration 516, loss = 0.76945166\n",
      "Iteration 517, loss = 0.77371193\n",
      "Iteration 518, loss = 0.76997799\n",
      "Iteration 519, loss = 0.76705387\n",
      "Iteration 520, loss = 0.76836485\n",
      "Iteration 521, loss = 0.76387098\n",
      "Iteration 522, loss = 0.76073215\n",
      "Iteration 523, loss = 0.76630672\n",
      "Iteration 524, loss = 0.78146402\n",
      "Iteration 525, loss = 0.77007383\n",
      "Iteration 526, loss = 0.76945396\n",
      "Iteration 527, loss = 0.76377152\n",
      "Iteration 528, loss = 0.76647003\n",
      "Iteration 529, loss = 0.76078117\n",
      "Iteration 530, loss = 0.76008532\n",
      "Iteration 531, loss = 0.76428333\n",
      "Iteration 532, loss = 0.77323872\n",
      "Iteration 533, loss = 0.75657577\n",
      "Iteration 534, loss = 0.75155413\n",
      "Iteration 535, loss = 0.74291176\n",
      "Iteration 536, loss = 0.74206159\n",
      "Iteration 537, loss = 0.74880327\n",
      "Iteration 538, loss = 0.73352884\n",
      "Iteration 539, loss = 0.73739586\n",
      "Iteration 540, loss = 0.75129282\n",
      "Iteration 541, loss = 0.73782453\n",
      "Iteration 542, loss = 0.73016279\n",
      "Iteration 543, loss = 0.73068811\n",
      "Iteration 544, loss = 0.71815373\n",
      "Iteration 545, loss = 0.71755517\n",
      "Iteration 546, loss = 0.71655091\n",
      "Iteration 547, loss = 0.71355356\n",
      "Iteration 548, loss = 0.72391319\n",
      "Iteration 549, loss = 0.71688307\n",
      "Iteration 550, loss = 0.71137363\n",
      "Iteration 551, loss = 0.71793580\n",
      "Iteration 552, loss = 0.71362393\n",
      "Iteration 553, loss = 0.71325844\n",
      "Iteration 554, loss = 0.71036713\n",
      "Iteration 555, loss = 0.71082544\n",
      "Iteration 556, loss = 0.71650090\n",
      "Iteration 557, loss = 0.70930809\n",
      "Iteration 558, loss = 0.71368972\n",
      "Iteration 559, loss = 0.70884111\n",
      "Iteration 560, loss = 0.69943937\n",
      "Iteration 561, loss = 0.70831388\n",
      "Iteration 562, loss = 0.70790402\n",
      "Iteration 563, loss = 0.69050554\n",
      "Iteration 564, loss = 0.68859944\n",
      "Iteration 565, loss = 0.69137342\n",
      "Iteration 566, loss = 0.68819880\n",
      "Iteration 567, loss = 0.68555069\n",
      "Iteration 568, loss = 0.68726344\n",
      "Iteration 569, loss = 0.68832678\n",
      "Iteration 570, loss = 0.68803246\n",
      "Iteration 571, loss = 0.68163998\n",
      "Iteration 572, loss = 0.68274315\n",
      "Iteration 573, loss = 0.67540907\n",
      "Iteration 574, loss = 0.67698031\n",
      "Iteration 575, loss = 0.67224406\n",
      "Iteration 576, loss = 0.67060726\n",
      "Iteration 577, loss = 0.67136099\n",
      "Iteration 578, loss = 0.66782025\n",
      "Iteration 579, loss = 0.66929081\n",
      "Iteration 580, loss = 0.66474779\n",
      "Iteration 581, loss = 0.66206893\n",
      "Iteration 582, loss = 0.66007104\n",
      "Iteration 583, loss = 0.65958597\n",
      "Iteration 584, loss = 0.65782159\n",
      "Iteration 585, loss = 0.65361454\n",
      "Iteration 586, loss = 0.65857212\n",
      "Iteration 587, loss = 0.65918668\n",
      "Iteration 588, loss = 0.66056077\n",
      "Iteration 589, loss = 0.65434196\n",
      "Iteration 590, loss = 0.64755921\n",
      "Iteration 591, loss = 0.64546767\n",
      "Iteration 592, loss = 0.64797062\n",
      "Iteration 593, loss = 0.64217152\n",
      "Iteration 594, loss = 0.64741551\n",
      "Iteration 595, loss = 0.64224338\n",
      "Iteration 596, loss = 0.64068039\n",
      "Iteration 597, loss = 0.64583354\n",
      "Iteration 598, loss = 0.64689044\n",
      "Iteration 599, loss = 0.63554105\n",
      "Iteration 600, loss = 0.63661636\n",
      "Iteration 601, loss = 0.63384455\n",
      "Iteration 602, loss = 0.63256475\n",
      "Iteration 603, loss = 0.62911779\n",
      "Iteration 604, loss = 0.63525137\n",
      "Iteration 605, loss = 0.63169737\n",
      "Iteration 606, loss = 0.63921907\n",
      "Iteration 607, loss = 0.63346287\n",
      "Iteration 608, loss = 0.63580587\n",
      "Iteration 609, loss = 0.62384879\n",
      "Iteration 610, loss = 0.63006173\n",
      "Iteration 611, loss = 0.64541087\n",
      "Iteration 612, loss = 0.63100378\n",
      "Iteration 613, loss = 0.62804339\n",
      "Iteration 614, loss = 0.62011664\n",
      "Iteration 615, loss = 0.61885030\n",
      "Iteration 616, loss = 0.61098272\n",
      "Iteration 617, loss = 0.60946139\n",
      "Iteration 618, loss = 0.60583752\n",
      "Iteration 619, loss = 0.60891178\n",
      "Iteration 620, loss = 0.60960547\n",
      "Iteration 621, loss = 0.60558298\n",
      "Iteration 622, loss = 0.60440567\n",
      "Iteration 623, loss = 0.59995108\n",
      "Iteration 624, loss = 0.59550808\n",
      "Iteration 625, loss = 0.59670046\n",
      "Iteration 626, loss = 0.59173086\n",
      "Iteration 627, loss = 0.59215069\n",
      "Iteration 628, loss = 0.59252936\n",
      "Iteration 629, loss = 0.59165722\n",
      "Iteration 630, loss = 0.58967654\n",
      "Iteration 631, loss = 0.58747270\n",
      "Iteration 632, loss = 0.58399253\n",
      "Iteration 633, loss = 0.58476271\n",
      "Iteration 634, loss = 0.58469219\n",
      "Iteration 635, loss = 0.58444233\n",
      "Iteration 636, loss = 0.58278911\n",
      "Iteration 637, loss = 0.58355566\n",
      "Iteration 638, loss = 0.58365590\n",
      "Iteration 639, loss = 0.59402226\n",
      "Iteration 640, loss = 0.59430048\n",
      "Iteration 641, loss = 0.58805393\n",
      "Iteration 642, loss = 0.57924618\n",
      "Iteration 643, loss = 0.58089178\n",
      "Iteration 644, loss = 0.58851837\n",
      "Iteration 645, loss = 0.58821871\n",
      "Iteration 646, loss = 0.57840079\n",
      "Iteration 647, loss = 0.57428463\n",
      "Iteration 648, loss = 0.56927301\n",
      "Iteration 649, loss = 0.58830867\n",
      "Iteration 650, loss = 0.56703479\n",
      "Iteration 651, loss = 0.57131804\n",
      "Iteration 652, loss = 0.56875077\n",
      "Iteration 653, loss = 0.56784064\n",
      "Iteration 654, loss = 0.56097884\n",
      "Iteration 655, loss = 0.56656202\n",
      "Iteration 656, loss = 0.56588225\n",
      "Iteration 657, loss = 0.55974786\n",
      "Iteration 658, loss = 0.56368382\n",
      "Iteration 659, loss = 0.55584680\n",
      "Iteration 660, loss = 0.56582990\n",
      "Iteration 661, loss = 0.55579931\n",
      "Iteration 662, loss = 0.55547431\n",
      "Iteration 663, loss = 0.55857711\n",
      "Iteration 664, loss = 0.55698108\n",
      "Iteration 665, loss = 0.55605701\n",
      "Iteration 666, loss = 0.55358321\n",
      "Iteration 667, loss = 0.57361530\n",
      "Iteration 668, loss = 0.54751257\n",
      "Iteration 669, loss = 0.54563852\n",
      "Iteration 670, loss = 0.53696112\n",
      "Iteration 671, loss = 0.54116770\n",
      "Iteration 672, loss = 0.54474739\n",
      "Iteration 673, loss = 0.53856783\n",
      "Iteration 674, loss = 0.53320335\n",
      "Iteration 675, loss = 0.53394657\n",
      "Iteration 676, loss = 0.53137052\n",
      "Iteration 677, loss = 0.52572358\n",
      "Iteration 678, loss = 0.52467319\n",
      "Iteration 679, loss = 0.52415142\n",
      "Iteration 680, loss = 0.52300246\n",
      "Iteration 681, loss = 0.51979309\n",
      "Iteration 682, loss = 0.52612423\n",
      "Iteration 683, loss = 0.52074901\n",
      "Iteration 684, loss = 0.52013390\n",
      "Iteration 685, loss = 0.51733668\n",
      "Iteration 686, loss = 0.51600283\n",
      "Iteration 687, loss = 0.51693773\n",
      "Iteration 688, loss = 0.51157667\n",
      "Iteration 689, loss = 0.51166676\n",
      "Iteration 690, loss = 0.51030218\n",
      "Iteration 691, loss = 0.50920863\n",
      "Iteration 692, loss = 0.50912102\n",
      "Iteration 693, loss = 0.50831731\n",
      "Iteration 694, loss = 0.50789597\n",
      "Iteration 695, loss = 0.50503617\n",
      "Iteration 696, loss = 0.51190293\n",
      "Iteration 697, loss = 0.50895595\n",
      "Iteration 698, loss = 0.50318243\n",
      "Iteration 699, loss = 0.50851135\n",
      "Iteration 700, loss = 0.50396712\n",
      "Iteration 701, loss = 0.50266940\n",
      "Iteration 702, loss = 0.49842542\n",
      "Iteration 703, loss = 0.50097851\n",
      "Iteration 704, loss = 0.50235774\n",
      "Iteration 705, loss = 0.49728411\n",
      "Iteration 706, loss = 0.50221293\n",
      "Iteration 707, loss = 0.49991393\n",
      "Iteration 708, loss = 0.49221810\n",
      "Iteration 709, loss = 0.50164027\n",
      "Iteration 710, loss = 0.51216327\n",
      "Iteration 711, loss = 0.50253672\n",
      "Iteration 712, loss = 0.50045027\n",
      "Iteration 713, loss = 0.50125521\n",
      "Iteration 714, loss = 0.49861787\n",
      "Iteration 715, loss = 0.49969452\n",
      "Iteration 716, loss = 0.49309595\n",
      "Iteration 717, loss = 0.49453294\n",
      "Iteration 718, loss = 0.48916312\n",
      "Iteration 719, loss = 0.48479511\n",
      "Iteration 720, loss = 0.48343073\n",
      "Iteration 721, loss = 0.47992788\n",
      "Iteration 722, loss = 0.47753831\n",
      "Iteration 723, loss = 0.47853905\n",
      "Iteration 724, loss = 0.48136445\n",
      "Iteration 725, loss = 0.47874287\n",
      "Iteration 726, loss = 0.47646108\n",
      "Iteration 727, loss = 0.47897381\n",
      "Iteration 728, loss = 0.46657246\n",
      "Iteration 729, loss = 0.46396958\n",
      "Iteration 730, loss = 0.46449356\n",
      "Iteration 731, loss = 0.46297305\n",
      "Iteration 732, loss = 0.46265437\n",
      "Iteration 733, loss = 0.46528704\n",
      "Iteration 734, loss = 0.46788773\n",
      "Iteration 735, loss = 0.45510877\n",
      "Iteration 736, loss = 0.45660391\n",
      "Iteration 737, loss = 0.45577138\n",
      "Iteration 738, loss = 0.45617721\n",
      "Iteration 739, loss = 0.45782229\n",
      "Iteration 740, loss = 0.45849619\n",
      "Iteration 741, loss = 0.45489471\n",
      "Iteration 742, loss = 0.45611781\n",
      "Iteration 743, loss = 0.45189033\n",
      "Iteration 744, loss = 0.45473390\n",
      "Iteration 745, loss = 0.45257519\n",
      "Iteration 746, loss = 0.45336049\n",
      "Iteration 747, loss = 0.46063686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 748, loss = 0.44821046\n",
      "Iteration 749, loss = 0.44966175\n",
      "Iteration 750, loss = 0.44853740\n",
      "Iteration 751, loss = 0.44794001\n",
      "Iteration 752, loss = 0.44923412\n",
      "Iteration 753, loss = 0.44770222\n",
      "Iteration 754, loss = 0.44136302\n",
      "Iteration 755, loss = 0.44358192\n",
      "Iteration 756, loss = 0.44275825\n",
      "Iteration 757, loss = 0.45493358\n",
      "Iteration 758, loss = 0.44951830\n",
      "Iteration 759, loss = 0.44944264\n",
      "Iteration 760, loss = 0.43983258\n",
      "Iteration 761, loss = 0.44330235\n",
      "Iteration 762, loss = 0.44271470\n",
      "Iteration 763, loss = 0.44074441\n",
      "Iteration 764, loss = 0.43660281\n",
      "Iteration 765, loss = 0.43654140\n",
      "Iteration 766, loss = 0.42819134\n",
      "Iteration 767, loss = 0.42884951\n",
      "Iteration 768, loss = 0.42453992\n",
      "Iteration 769, loss = 0.42336580\n",
      "Iteration 770, loss = 0.42289784\n",
      "Iteration 771, loss = 0.42416134\n",
      "Iteration 772, loss = 0.42366573\n",
      "Iteration 773, loss = 0.42017450\n",
      "Iteration 774, loss = 0.42238400\n",
      "Iteration 775, loss = 0.42207332\n",
      "Iteration 776, loss = 0.42498854\n",
      "Iteration 777, loss = 0.41996331\n",
      "Iteration 778, loss = 0.42181040\n",
      "Iteration 779, loss = 0.43040911\n",
      "Iteration 780, loss = 0.45038488\n",
      "Iteration 781, loss = 0.44238283\n",
      "Iteration 782, loss = 0.44189214\n",
      "Iteration 783, loss = 0.44817906\n",
      "Iteration 784, loss = 0.44191206\n",
      "Iteration 785, loss = 0.43506494\n",
      "Iteration 786, loss = 0.44610075\n",
      "Iteration 787, loss = 0.45923231\n",
      "Iteration 788, loss = 0.44094083\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c17&4b0c18_1200.csv\n",
      "682 581\n",
      "Any one :  0.8519061583577713\n",
      "At position 1 and 2 :  0.7434017595307918\n",
      "At position 1 :  0.532258064516129\n",
      "At position 2 :  0.21114369501466276\n",
      "At position 3 :  0.10850439882697947\n",
      "training data :  0.6270190895741556\n",
      "0.532258064516129\n",
      "2451600 1200000\n",
      "Iteration 789, loss = 0.87791972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c17&4b0c18_1200.csv\n",
      "682 579\n",
      "Any one :  0.8489736070381232\n",
      "At position 1 and 2 :  0.7404692082111437\n",
      "At position 1 :  0.5205278592375366\n",
      "At position 2 :  0.21994134897360704\n",
      "At position 3 :  0.10850439882697947\n",
      "training data :  0.6260401370533529\n",
      "0.5205278592375366\n",
      "2451600 1800000\n",
      "Iteration 790, loss = 1.00982585\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c17&4b0c18_1200.csv\n",
      "682 583\n",
      "Any one :  0.8548387096774194\n",
      "At position 1 and 2 :  0.7536656891495601\n",
      "At position 1 :  0.5219941348973607\n",
      "At position 2 :  0.2316715542521994\n",
      "At position 3 :  0.10117302052785923\n",
      "training data :  0.6098874204601077\n",
      "0.5219941348973607\n",
      "2451600 2400000\n",
      "Iteration 791, loss = 1.09399633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c17&4b0c18_1200.csv\n",
      "682 583\n",
      "Any one :  0.8548387096774194\n",
      "At position 1 and 2 :  0.749266862170088\n",
      "At position 1 :  0.5293255131964809\n",
      "At position 2 :  0.21994134897360704\n",
      "At position 3 :  0.10557184750733138\n",
      "training data :  0.6284875183553598\n",
      "0.5293255131964809\n",
      "2451600 2451600\n",
      "Iteration 792, loss = 1.09671692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c17&4b0c18_1200.csv\n",
      "682 564\n",
      "Any one :  0.8269794721407625\n",
      "At position 1 and 2 :  0.7052785923753666\n",
      "At position 1 :  0.5029325513196481\n",
      "At position 2 :  0.20234604105571846\n",
      "At position 3 :  0.1217008797653959\n",
      "training data :  0.5849241311796378\n",
      "0.5029325513196481\n",
      "2451600 2451600\n",
      "Iteration 793, loss = 1.12472116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c17&4b0c18_1200.csv\n",
      "682 585\n",
      "Any one :  0.8577712609970675\n",
      "At position 1 and 2 :  0.7478005865102639\n",
      "At position 1 :  0.5161290322580645\n",
      "At position 2 :  0.2316715542521994\n",
      "At position 3 :  0.10997067448680352\n",
      "training data :  0.6005873715124816\n",
      "0.5161290322580645\n",
      "272\n",
      "1\n",
      "244800 244800\n",
      "Iteration 1, loss = 2.37575938\n",
      "Iteration 2, loss = 2.32248138\n",
      "Iteration 3, loss = 2.31874101\n",
      "Iteration 4, loss = 2.31739426\n",
      "Iteration 5, loss = 2.31696613\n",
      "Iteration 6, loss = 2.31707544\n",
      "Iteration 7, loss = 2.31659062\n",
      "Iteration 8, loss = 2.31613761\n",
      "Iteration 9, loss = 2.31631267\n",
      "Iteration 10, loss = 2.31635440\n",
      "Iteration 11, loss = 2.31623436\n",
      "Iteration 12, loss = 2.31591384\n",
      "Iteration 13, loss = 2.31564279\n",
      "Iteration 14, loss = 2.31527470\n",
      "Iteration 15, loss = 2.31460596\n",
      "Iteration 16, loss = 2.31428321\n",
      "Iteration 17, loss = 2.31445387\n",
      "Iteration 18, loss = 2.31433306\n",
      "Iteration 19, loss = 2.31436790\n",
      "Iteration 20, loss = 2.31469450\n",
      "Iteration 21, loss = 2.31460333\n",
      "Iteration 22, loss = 2.31450425\n",
      "Iteration 23, loss = 2.31463590\n",
      "Iteration 24, loss = 2.31465229\n",
      "Iteration 25, loss = 2.31429622\n",
      "Iteration 26, loss = 2.31391517\n",
      "Iteration 27, loss = 2.31378717\n",
      "Iteration 28, loss = 2.31346465\n",
      "Iteration 29, loss = 2.31363451\n",
      "Iteration 30, loss = 2.31382249\n",
      "Iteration 31, loss = 2.31398109\n",
      "Iteration 32, loss = 2.31357976\n",
      "Iteration 33, loss = 2.31325332\n",
      "Iteration 34, loss = 2.31283630\n",
      "Iteration 35, loss = 2.31232567\n",
      "Iteration 36, loss = 2.31198869\n",
      "Iteration 37, loss = 2.31173851\n",
      "Iteration 38, loss = 2.31243153\n",
      "Iteration 39, loss = 2.31333557\n",
      "Iteration 40, loss = 2.31357959\n",
      "Iteration 41, loss = 2.31357887\n",
      "Iteration 42, loss = 2.31314037\n",
      "Iteration 43, loss = 2.31242717\n",
      "Iteration 44, loss = 2.31169057\n",
      "Iteration 45, loss = 2.31132475\n",
      "Iteration 46, loss = 2.31109705\n",
      "Iteration 47, loss = 2.31112326\n",
      "Iteration 48, loss = 2.31099992\n",
      "Iteration 49, loss = 2.31060998\n",
      "Iteration 50, loss = 2.30961084\n",
      "Iteration 51, loss = 2.30830157\n",
      "Iteration 52, loss = 2.30775878\n",
      "Iteration 53, loss = 2.30710454\n",
      "Iteration 54, loss = 2.30639911\n",
      "Iteration 55, loss = 2.30581588\n",
      "Iteration 56, loss = 2.30526411\n",
      "Iteration 57, loss = 2.30500078\n",
      "Iteration 58, loss = 2.30468278\n",
      "Iteration 59, loss = 2.30388960\n",
      "Iteration 60, loss = 2.30350459\n",
      "Iteration 61, loss = 2.30325367\n",
      "Iteration 62, loss = 2.30263265\n",
      "Iteration 63, loss = 2.30202144\n",
      "Iteration 64, loss = 2.30199003\n",
      "Iteration 65, loss = 2.30209196\n",
      "Iteration 66, loss = 2.30157759\n",
      "Iteration 67, loss = 2.30141247\n",
      "Iteration 68, loss = 2.30222212\n",
      "Iteration 69, loss = 2.30128153\n",
      "Iteration 70, loss = 2.29952886\n",
      "Iteration 71, loss = 2.29944451\n",
      "Iteration 72, loss = 2.29963993\n",
      "Iteration 73, loss = 2.29976818\n",
      "Iteration 74, loss = 2.29928772\n",
      "Iteration 75, loss = 2.29852666\n",
      "Iteration 76, loss = 2.29784794\n",
      "Iteration 77, loss = 2.29774239\n",
      "Iteration 78, loss = 2.29770173\n",
      "Iteration 79, loss = 2.29757693\n",
      "Iteration 80, loss = 2.29648618\n",
      "Iteration 81, loss = 2.29482944\n",
      "Iteration 82, loss = 2.29365654\n",
      "Iteration 83, loss = 2.29279623\n",
      "Iteration 84, loss = 2.29297546\n",
      "Iteration 85, loss = 2.29286043\n",
      "Iteration 86, loss = 2.29236540\n",
      "Iteration 87, loss = 2.29125923\n",
      "Iteration 88, loss = 2.29133025\n",
      "Iteration 89, loss = 2.29013122\n",
      "Iteration 90, loss = 2.29005320\n",
      "Iteration 91, loss = 2.29050203\n",
      "Iteration 92, loss = 2.28906474\n",
      "Iteration 93, loss = 2.28686584\n",
      "Iteration 94, loss = 2.28837626\n",
      "Iteration 95, loss = 2.28819924\n",
      "Iteration 96, loss = 2.28945718\n",
      "Iteration 97, loss = 2.29061081\n",
      "Iteration 98, loss = 2.29091182\n",
      "Iteration 99, loss = 2.29076942\n",
      "Iteration 100, loss = 2.29284522\n",
      "Iteration 101, loss = 2.29640223\n",
      "Iteration 102, loss = 2.29252456\n",
      "Iteration 103, loss = 2.28988282\n",
      "Iteration 104, loss = 2.28921176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acc_wGyro_400_window_4b0c19_1200.csv\n",
      "69 27\n",
      "Any one :  0.391304347826087\n",
      "At position 1 and 2 :  0.3333333333333333\n",
      "At position 1 :  0.2463768115942029\n",
      "At position 2 :  0.08695652173913043\n",
      "At position 3 :  0.057971014492753624\n",
      "training data :  0.19607843137254902\n",
      "0.2463768115942029\n"
     ]
    }
   ],
   "source": [
    "for file in pathex:\n",
    "    iters=500\n",
    "    ni=file.rfind(\"_\")+1\n",
    "    n=int(file[ni:].split(\".\")[0])\n",
    "    dfuni=pd.read_csv(path+file)\n",
    "    last=ceil(dfuni.index[-1]/iters)\n",
    "    print(dfuni.index[-1])\n",
    "    print(last)\n",
    "\n",
    "#     fpath=\"C:\\\\Users\\\\nilu1\\\\Desktop\\\\Code\\\\AndroidPattern\\\\Data\\\\Results\\\\\"\n",
    "#     f = open(fpath+file+\"idenhide\"+\".txt\", \"w\")\n",
    "#     f.write(\"\\n\"+str(dfuni.index[-1]))\n",
    "#     f.write(\"\\n\"+str(last))\n",
    "    \n",
    "#     fl = open(fpath+file+\"loghide\"+\".txt\", \"w\")\n",
    "#     fl.write(\"\\n\"+str(dfuni.index[-1]))\n",
    "#     fl.write(\"\\n\"+str(last))\n",
    "\n",
    "    X = dfuni.iloc[:, 0:n].values\n",
    "    y = dfuni.iloc[:, n].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    logmodel=MLPClassifier(solver=\"adam\",max_iter=100,warm_start=True,activation=\"identity\",hidden_layer_sizes=(400,10),verbose=True)\n",
    "#     logmodell=MLPClassifier(solver=\"lbfgs\",max_iter=500,warm_start=True,activation=\"logistic\",hidden_layer_sizes=(400,10))\n",
    "\n",
    "    for i in range(last):\n",
    "\n",
    "        if(i!=last-1):\n",
    "            X = X_train[:iters*(i+1)]\n",
    "            y = y_train[:iters*(i+1)]\n",
    "        else:\n",
    "            X=X_train\n",
    "            y=y_train\n",
    "\n",
    "        print(X_train.size,X.size)\n",
    "#         f.write(\"\\n\"+str(X_train.size)+str(X.size))\n",
    "#         fl.write(\"\\n\"+str(X_train.size)+str(X.size))\n",
    "\n",
    "        logmodel.fit(X, y)\n",
    "#         logmodell.fit(X, y)\n",
    "        y_pred = logmodel.predict(X_train)\n",
    "        trainprob=accuracy_score(y_train, y_pred)\n",
    "        \n",
    "        y_pred = logmodel.predict(X_test)\n",
    "#         y_predl = logmodell.predict(X_test)\n",
    "\n",
    "        p = logmodel.predict_proba(X_test)\n",
    "        nn = 3\n",
    "        top_n = np.argsort(p)[:,:-nn-1:-1]\n",
    "        #     for i in range(len(y_test)):\n",
    "        #         print(y_test[i],top_n[i])\n",
    "        count=0\n",
    "        count1=0\n",
    "        count2=0\n",
    "        count3=0\n",
    "        count12=0\n",
    "        for i in range(len(y_pred)):\n",
    "            if(y_test[i] in top_n[i]):\n",
    "                count+=1\n",
    "            if(y_test[i] in top_n[i][:2]):\n",
    "                count12+=1\n",
    "            if(y_test[i] == top_n[i][0]):\n",
    "                count1+=1\n",
    "            elif(y_test[i] == top_n[i][1]):\n",
    "                count2+=1\n",
    "            elif(y_test[i] == top_n[i][2]):\n",
    "                count3+=1\n",
    "        #         print(y_test[i],y_pred[i])\n",
    "        l=len(y_pred)\n",
    "        print(file)\n",
    "        print(l,count)\n",
    "        print(\"Any one : \",count/l)\n",
    "        print(\"At position 1 and 2 : \",count12/l)\n",
    "        print(\"At position 1 : \",count1/l)\n",
    "        print(\"At position 2 : \",count2/l)\n",
    "        print(\"At position 3 : \",count3/l)\n",
    "        print(\"training data : \",trainprob)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#         f.write(\"\\n\"+file)\n",
    "#         f.write(\"\\n\"+str(l)+str(count))\n",
    "#         f.write(\"\\nAny one : \"+str(count/l))\n",
    "#         f.write(\"\\nAt position 1 and 2 : \"+str(count12/l))\n",
    "#         f.write(\"\\nAt position 1 : \"+str(count1/l))\n",
    "#         f.write(\"\\nAt position 2 : \"+str(count2/l))\n",
    "#         f.write(\"\\nAt position 3 : \"+str(count3/l))\n",
    "#         f.write(\"\\n\"+str(accuracy_score(y_test, y_pred)))\n",
    "        \n",
    "#         p = logmodell.predict_proba(X_test)\n",
    "#         nn = 3\n",
    "#         top_n = np.argsort(p)[:,:-nn-1:-1]\n",
    "#         count=0\n",
    "#         count1=0\n",
    "#         count2=0\n",
    "#         count3=0\n",
    "#         count12=0\n",
    "#         for i in range(len(y_predl)):\n",
    "#             if(y_test[i] in top_n[i]):\n",
    "#                 count+=1\n",
    "#             if(y_test[i] in top_n[i][:2]):\n",
    "#                 count12+=1\n",
    "#             if(y_test[i] == top_n[i][0]):\n",
    "#                 count1+=1\n",
    "#             elif(y_test[i] == top_n[i][1]):\n",
    "#                 count2+=1\n",
    "#             elif(y_test[i] == top_n[i][2]):\n",
    "#                 count3+=1\n",
    "#         #         print(y_test[i],y_pred[i])\n",
    "#         l=len(y_predl)\n",
    "#         print(file)\n",
    "#         print(l,count)\n",
    "#         print(\"Any one : \",count/l)\n",
    "#         print(\"At position 1 and 2 : \",count12/l)\n",
    "#         print(\"At position 1 : \",count1/l)\n",
    "#         print(\"At position 2 : \",count2/l)\n",
    "#         print(\"At position 3 : \",count3/l)\n",
    "#         print(accuracy_score(y_test, y_predl))\n",
    "\n",
    "#         fl.write(\"\\n\"+file)\n",
    "#         fl.write(\"\\n\"+str(l)+str(count))\n",
    "#         fl.write(\"\\nAny one : \"+str(count/l))\n",
    "#         fl.write(\"\\nAt position 1 and 2 : \"+str(count12/l))\n",
    "#         fl.write(\"\\nAt position 1 : \"+str(count1/l))\n",
    "#         fl.write(\"\\nAt position 2 : \"+str(count2/l))\n",
    "#         fl.write(\"\\nAt position 3 : \"+str(count3/l))\n",
    "#         fl.write(\"\\n\"+str(accuracy_score(y_test, y_predl)))\n",
    "\n",
    "# #     f.close()\n",
    "#     fl.close()\n",
    "    \n",
    "# filename = \"zmlp500_\"+file[:-4]+\".sav\"\n",
    "# pickle.dump(logmodel, open(modelpath+filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
